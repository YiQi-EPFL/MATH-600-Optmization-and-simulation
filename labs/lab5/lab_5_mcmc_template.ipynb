{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisation & Simulation: Lab on MCMC methods\n",
    "\n",
    "**Objective:**\n",
    "- Use Markov chain Monte Carlo (MCMC) methods to draw from complex distributions.\n",
    "\n",
    "**Implementation:**\n",
    "- Gibbs sampling\n",
    "- Metropolis-Hastings algorithm\n",
    "\n",
    "**Agenda:**\n",
    "1. Implement Gibbs sampler to generate draws from bivariate normal.\n",
    "2. Implement Metropolis-Hastings algorithms to infer posterior distribution of parameters of Cauchy distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gibbs sampling\n",
    "\n",
    "### Background\n",
    "\n",
    "**Motivation and intuition:**\n",
    "- Draw from multivariate distributions from which direct sampling is difficult.\n",
    "- Construct conditional distributions from which sampling is easy. \n",
    "- Iteratively draw from conditional distributions. \n",
    "\n",
    "\n",
    "- Suppose we wish to sample $\\theta_{1}, \\theta_{2} \\sim p(\\theta_{1}, \\theta_{2})$ but cannot do so directly. \n",
    "- However, we can sample $\\theta_{1} \\sim p(\\theta_{1} \\vert \\theta_{2})$ and $\\theta_{2} \\sim p(\\theta_{2} \\vert \\theta_{1})$.\n",
    "\n",
    "**Algorithm:**\n",
    "1. Set $j = 0$. \n",
    "2. Provide initial values $(\\theta_{1}^{(0)}, \\theta_{2}^{(0)})$. \n",
    "3. Set $j = j + 1$. \n",
    "4. $\\theta_{1}^{(j)} \\sim p(\\theta_{1}^{(j)} \\vert \\theta_{2}^{(j-1)})$.\n",
    "5. $\\theta_{2}^{(j)} \\sim p(\\theta_{2}^{(j)} \\vert \\theta_{1}^{(j)})$.\n",
    "6. If $j$ is less than the desired number of draws, return to step 3.\n",
    "\n",
    "### Exercise\n",
    "- Implement a Gibbs sampler to draw from a bivariate normal, i.e. \n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\theta_{1} \\\\ \\theta_{2}\n",
    "\\end{pmatrix}\n",
    "\\sim\n",
    "\\mathcal{N} (\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\n",
    "\\text{ with }\n",
    "\\boldsymbol{\\mu} = \\boldsymbol{0},\n",
    "\\boldsymbol{\\Sigma} = \n",
    "\\begin{pmatrix}\n",
    "1 & \\rho \\\\\n",
    "\\rho & 1 \n",
    "\\end{pmatrix},\n",
    "\\rho = 0.8\n",
    "$$\n",
    "- Visually compare the empirical density of the draws to the theoretical density of the sampling distribution. \n",
    "\n",
    "**Note:**\n",
    "- If\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\theta_{1} \\\\ \\theta_{2}\n",
    "\\end{pmatrix}\n",
    "\\sim\n",
    "\\mathcal{N} \\left (\\boldsymbol{0}, \n",
    "\\begin{pmatrix}\n",
    "1 & \\rho \\\\\n",
    "\\rho & 1 \n",
    "\\end{pmatrix}\n",
    "\\right ),\n",
    "$$\n",
    "then \n",
    "$$\n",
    "\\theta_{1} \\vert \\theta_{2} \\sim \\mathcal{N}(\\rho \\theta_{2}, 1 - \\rho^{2}),\n",
    "$$\n",
    "$$\n",
    "\\theta_{2} \\vert \\theta_{1} \\sim \\mathcal{N}(\\rho \\theta_{1}, 1 - \\rho^{2}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs(rho, theta2, n_draws):\n",
    "    \"\"\" Gibbs sampling of bivariate normal.\n",
    "    \n",
    "    Keywords:\n",
    "        rho (float): correlation parameter.\n",
    "        theta2 (float): initial value of theta2.\n",
    "        n_draws (int): number of draws.\n",
    "    \n",
    "    Returns:\n",
    "        theta (array): draws from bivariate normal.\n",
    "    \"\"\"\n",
    "    \n",
    "    ###\n",
    "    #ADD YOUR CODE HERE.\n",
    "    ###\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho = 0.8\n",
    "theta2 = 0\n",
    "n_draws = 1000\n",
    "\n",
    "draws = gibbs(rho, theta2, n_draws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.subplot(1,1,1)\n",
    "\n",
    "#Theoretical density\n",
    "mu = np.array([0, 0])\n",
    "Sigma = np.array([[1, rho], [rho, 1]])\n",
    "x = np.arange(-3,3,0.01)\n",
    "xx, yy = np.meshgrid(x,x)\n",
    "z = multivariate_normal.pdf(np.stack((xx, yy), axis=2), mu, Sigma)\n",
    "ax.contour(x, x, z)\n",
    "\n",
    "#Empirical draws\n",
    "ax = sns.kdeplot(\n",
    "    draws[0,:], y=draws[1,:],\n",
    "    cmap= \"Blues\" , shade = True\n",
    ")\n",
    "\n",
    "ax.set(\n",
    "    xlabel=r'$\\theta_1$',\n",
    "    ylabel=r'$\\theta_2$'\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metropolis-Hastings algorithm\n",
    "\n",
    "### Background\n",
    "\n",
    "**Motivation and intuition:**\n",
    "- Gibbs sampling is only feasible, if the conditional distributions are known and if it is easy to sample from them. \n",
    "- The Metropolis-Hastings algorithm allows us to sample from any density $p(\\theta)$, provided that we can evaluate $f(\\theta)$, a density that is proportional to $p$. \n",
    "- At each iteration, we sample a new state from a candidate distribution, which depends on the current state. \n",
    "- With some probability depending on the value of the target density at the new and the current states, the new state is accepted. \n",
    "- As the algorithm proceeds, the sampled states approximate the desired density. \n",
    "\n",
    "**Algorithm:**\n",
    "1. Set $j = 0$.\n",
    "2. Let $\\theta$ denote the current state.\n",
    "3. Let $p(\\theta \\vert x)$ denote the target density.\n",
    "4. Let $J(\\theta^{*} \\vert \\theta)$ denote the jumping distribution\n",
    "5. Let $\\rho$ denote the step size.\n",
    "6. Set $j = j + 1$. \n",
    "7. Propose a new state $\\theta^{*} \\vert \\theta \\sim J(\\theta)$.\n",
    "8. Calculate $\\alpha = \\frac{p(\\theta^{*} \\vert x)}{p(\\theta \\vert x)} \\cdot \\frac{J(\\theta \\vert \\theta^{*})}{J(\\theta^{*} \\vert \\theta)}$.\n",
    "9. Draw $r \\sim \\text{Uniform}(0,1)$.\n",
    "10. If $r \\leq \\alpha$, accept the new state and set $\\theta = \\theta^{*}$. Otherwise, reject the new state. \n",
    "11. If $j$ is less than the desired number of draws, return to step 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise \n",
    "- Simulate data from Cauchy distribution with location $\\mu = 1$ and scale $\\gamma = 1$.\n",
    "- Implement a Metroplis-Hastings algorithms to infer the posterior distribution of the scale parameter of the Cauchy distribution.\n",
    "- Consider two possible jumping distributions:\n",
    "- 1. $\\text{Normal}(\\theta, \\rho^2)$, where $\\rho$ is the step size.\n",
    "- 2. $\\text{Lognormal}(\\log(\\theta) - 0.5 \\rho^2, \\rho)$, where $\\rho$ is a distance parameter. Note that $\\text{Lognormal}(\\mu, \\sigma)$ denotes a lognormal distribution with location $\\mu$ and scale $\\sigma$.\n",
    "- Evaluate the performance of the algorithms for different parametrisations of the jumping distributions. Compute the potential scale reduction factors $\\hat{R}$ and the effective sample size $n_{\\text{eff}}$.\n",
    "\n",
    "Suppose\n",
    "- $x_{i} \\sim \\mathcal{C}(\\mu, \\gamma)$ for $i \\in \\{1, \\dots, N\\}$\n",
    "- $\\gamma \\sim \\text{Gamma}(\\alpha_{0}, \\beta_{0})$ with $\\alpha_{0} = \\beta_{0} = 0.001$.\n",
    "\n",
    "Then\n",
    "$$\n",
    "p(\\gamma \\vert x, \\mu, \\alpha_{0}, \\beta_{0}) \n",
    "\\propto \\left ( \\prod_{i} p(x_{i} \\vert \\mu, \\gamma) \\right ) p(\\gamma \\vert \\alpha_{0}, \\beta_{0})\n",
    "$$\n",
    "\n",
    "**Note:**\n",
    "- PDF of Cauchy distribution:\n",
    "$$\n",
    "f(x \\vert \\mu, \\gamma) = \\frac{1}{\\pi \\gamma \\left [ 1 + \\left (\\frac{x - \\mu}{\\gamma} \\right )^{2} \\right ]}\n",
    "$$\n",
    "- Quantile function of Cauchy distribution:\n",
    "$$\n",
    "Q = \\mu + \\gamma \\cdot \\text{tan}\\left [ \\pi \\left ( F - \\frac{1}{2} \\right ) \\right ]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cauchy random draws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cauchy_rng(mu, gamma, n_draws):\n",
    "    \"\"\" Generate cauchy random numbers.\n",
    "    \n",
    "    Keywords:\n",
    "        mu (float): location parameter.\n",
    "        gamma (float): scale parameter.\n",
    "        n_draws (int): number of draws.\n",
    "    \n",
    "    Returns:\n",
    "        r (array): n_draws draws from Cauchy(mu, gamma).\n",
    "    \"\"\"\n",
    "    u = np.random.rand(n_draws,)\n",
    "    r = mu + gamma * np.tan(np.pi * (u - 0.5))\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 1\n",
    "gamma_true = 1\n",
    "n_draws = 1000\n",
    "x = cauchy_rng(mu, gamma_true, n_draws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal jumping distribution\n",
    "##### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rhat_n_eff(draws):\n",
    "    \"\"\" Computes potential scale reduction factor and effective sample size.\n",
    "    \n",
    "    Keywords:\n",
    "        draws (1d-array): draws\n",
    "    \n",
    "    Returns:\n",
    "        R (float): potential scale reduction factor.\n",
    "        n_eff (float): effective sample size.\n",
    "    \"\"\"\n",
    "    #Reshape draws\n",
    "    psi_cd = np.array(draws).reshape(2,-1)\n",
    "    C, D = psi_cd.shape\n",
    "    \n",
    "    #Rhat\n",
    "    psi_c = psi_cd.mean(axis=1, keepdims=True)\n",
    "    psi = psi_c.mean()\n",
    "    B = D / (C - 1) * np.sum((psi_c - psi)**2) #Between-chain variance\n",
    "    s2_c = 1 / (D - 1) * np.sum((psi_cd - psi_c)**2, axis=1)\n",
    "    W = s2_c.mean() #Within-chain variance\n",
    "    var = (D - 1) / D * W + 1 / D * B\n",
    "    R = np.sqrt(var / W)\n",
    "    \n",
    "    #Effective sample size\n",
    "    V = lambda t: ((psi_cd[:,t:] - psi_cd[:,:(D-t)])**2).sum() / (C * (D - t)) #Variogram\n",
    "    negative_autocorr = False\n",
    "    t = 1\n",
    "    rho = np.ones(D)\n",
    "    while not negative_autocorr and (t < D):\n",
    "        rho[t] = 1 - V(t) / (2 * var)\n",
    "        if not t % 2:\n",
    "            negative_autocorr = np.sum(rho[t-1:t+1]) < 0\n",
    "        t += 1\n",
    "    n_eff = C * D / (1 + 2 * np.sum(rho[1:t]))\n",
    "    \n",
    "    return R, n_eff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcmc_normal(x, mu, gamma, alpha0, beta0, rho, n_burn, n_keep):\n",
    "    \"\"\" MCMC algorithm with normal jumping distribution to estimate posterior of Cauchy scale parameter.\n",
    "    \n",
    "    Keywords:\n",
    "        x (array): data.\n",
    "        mu (float): Cauchy location parameter.\n",
    "        gamma (float): Cauchy scale parameter.\n",
    "        alpha0 (float): Gamma shape, gamma ~ Gamma(alpha0, beta0).\n",
    "        beta0 (float): Gamma rate, gamma ~ Gamma(alpha0, beta0).\n",
    "        rho (float): step size.\n",
    "        n_burn (int): burn in draws.\n",
    "        n_keep (int): number of draws to keep after burn-in. \n",
    "    \n",
    "    Returns:\n",
    "        gamma_store (array): gamma draws.\n",
    "        accept_store (array): accept flags.\n",
    "    \"\"\"\n",
    "    \n",
    "    ###\n",
    "    #ADD YOUR CODE HERE.\n",
    "    ###\n",
    "            \n",
    "    return gamma_store, accept_store     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 1\n",
    "gamma = 0.1\n",
    "alpha0 = 0.001\n",
    "beta0 = 0.001\n",
    "rho = 0.1\n",
    "n_burn = 1000\n",
    "n_keep = 1000\n",
    "\n",
    "gamma_store, accept_store = mcmc_normal(x, mu, gamma, alpha0, beta0, rho, n_burn, n_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_store.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_store.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accept_store.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhat_n_eff(gamma_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho_test = [0.00001, 0.0001, 0.001, 0.01, 0.1, 0.15, 0.2, 0.25, 0.5, 1]\n",
    "\n",
    "gamma_store_all = {}\n",
    "gamma_mean = []\n",
    "gamma_std = []\n",
    "gamma_accept = []\n",
    "gamma_rhat = []\n",
    "gamma_n_eff = []\n",
    "\n",
    "for rho in rho_test:\n",
    "    gamma_store, accept_store = mcmc_normal(x, mu, gamma, alpha0, beta0, rho, n_burn, n_keep)\n",
    "    rhat, n_eff = rhat_n_eff(gamma_store)\n",
    "    \n",
    "    gamma_store_all[rho] = gamma_store\n",
    "    gamma_mean.append(gamma_store.mean())\n",
    "    gamma_std.append(gamma_store.std())\n",
    "    gamma_accept.append(accept_store.mean())\n",
    "    gamma_rhat.append(rhat)\n",
    "    gamma_n_eff.append(n_eff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(\n",
    "    data={\n",
    "        'rho': rho_test,\n",
    "        'Post. mean': gamma_mean,\n",
    "        'Post. std.': gamma_std,\n",
    "        'Acceptance ratio': gamma_accept,\n",
    "        'Rhat': gamma_rhat,\n",
    "        'n_eff': gamma_n_eff\n",
    "    }\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trace plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "ax.plot(gamma_store_all[1], label = 'Chain 1')\n",
    "plt.axhline(gamma_true, color=\"black\", linestyle=\"--\", label='Truth')\n",
    "\n",
    "ax.set(xlabel = 'Iteration after burn in', ylabel = r'$\\gamma$',\n",
    "       title= r'Trace plot of posterior draws of $\\gamma$')\n",
    "plt.legend()\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lognormal jumping distribution\n",
    "##### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcmc_lognormal(x, mu, gamma, alpha0, beta0, rho, n_burn, n_keep):\n",
    "    \"\"\" MCMC algorithm with lognormal jumping distribution to estimate posterior of Cauchy scale parameter.\n",
    "    \n",
    "    Keywords:\n",
    "        x (array): data.\n",
    "        mu (float): Cauchy location parameter.\n",
    "        gamma (float): Cauchy scale parameter.\n",
    "        alpha0 (float): Gamma shape, gamma ~ Gamma(alpha0, beta0).\n",
    "        beta0 (float): Gamma rate, gamma ~ Gamma(alpha0, beta0).\n",
    "        rho (float): step size.\n",
    "        n_burn (int): burn in draws.\n",
    "        n_keep (int): number of draws to keep after burn-in. \n",
    "    \n",
    "    Returns:\n",
    "        gamma_store (array): gamma draws.\n",
    "        accept_store (array): accept flags.\n",
    "    \"\"\"\n",
    "    \n",
    "    ###\n",
    "    #ADD YOUR CODE HERE.\n",
    "    ###\n",
    "            \n",
    "    return gamma_store, accept_store     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 1\n",
    "gamma = 0.1\n",
    "alpha0 = 0.001\n",
    "beta0 = 0.001\n",
    "rho = 0.1\n",
    "n_burn = 1000\n",
    "n_keep = 1000\n",
    "\n",
    "gamma_store, accept_store = mcmc_lognormal(x, mu, gamma, alpha0, beta0, rho, n_burn, n_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_store.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_store.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accept_store.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhat_n_eff(gamma_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho_test = [0.00001, 0.0001, 0.001, 0.01, 0.1, 0.15, 0.2, 0.25, 0.5, 1]\n",
    "\n",
    "gamma_store_all = {}\n",
    "gamma_mean = []\n",
    "gamma_std = []\n",
    "gamma_accept = []\n",
    "gamma_rhat = []\n",
    "gamma_n_eff = []\n",
    "\n",
    "for rho in rho_test:\n",
    "    gamma_store, accept_store = mcmc_lognormal(x, mu, gamma, alpha0, beta0, rho, n_burn, n_keep)\n",
    "    rhat, n_eff = rhat_n_eff(gamma_store)\n",
    "    \n",
    "    gamma_store_all[rho] = gamma_store\n",
    "    gamma_mean.append(gamma_store.mean())\n",
    "    gamma_std.append(gamma_store.std())\n",
    "    gamma_accept.append(accept_store.mean())\n",
    "    gamma_rhat.append(rhat)\n",
    "    gamma_n_eff.append(n_eff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(\n",
    "    data={\n",
    "        'rho': rho_test,\n",
    "        'Post. mean': gamma_mean,\n",
    "        'Post. std.': gamma_std,\n",
    "        'Acceptance ratio': gamma_accept,\n",
    "        'Rhat': gamma_rhat,\n",
    "        'n_eff': gamma_n_eff\n",
    "    }\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trace plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "ax.plot(gamma_store_all[0.1], label = 'Chain 1')\n",
    "plt.axhline(gamma_true, color=\"black\", linestyle=\"--\", label='Truth')\n",
    "\n",
    "ax.set(xlabel = 'Iteration after burn in', ylabel = r'$\\gamma$',\n",
    "       title= r'Trace plot of posterior draws of $\\gamma$')\n",
    "plt.legend()\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
